{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import keras.utils\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input,Dense, Flatten\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Conv2D, UpSampling2D, AveragePooling2D, BatchNormalization\n",
    "from keras.layers import LeakyReLU, Reshape\n",
    "from keras.datasets import mnist\n",
    "from keras.models import save_model\n",
    "from keras import backend as K\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three helper functions\n",
    "1. generateTheta(L, dim) - generate L random samples from unit 'dim' space\n",
    "2. generateQZ(batchsize, latent_dim) - generate samples from ideal q_z distribution\n",
    "3. stitchImages(I, axis) - helps with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTheta(L, dim):\n",
    "    res = [w / np.sqrt((w ** 2).sum()) for w in np.random.normal(size=(L, dim))]\n",
    "    return np.asarray(res)\n",
    "\n",
    "def generateQZ(batchsize, latent_dim):\n",
    "    return np.random.normal(size=(batchsize, latent_dim))\n",
    "\n",
    "def stitchImages(I, axis=0):\n",
    "    n, N, M, K = I.shape\n",
    "    if axis == 0:\n",
    "        img = np.zeros((N * n, M, K))\n",
    "        for i in range(n):\n",
    "            img[i * N: (i + 1) * N, :, :] = I[i, :, :, :]\n",
    "    else:\n",
    "        img = np.zeros((N, M * n, K))\n",
    "        for i in range(n):\n",
    "            img[:, i * M: (i + 1) * M, :] = I[i, :, :, :]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General layers/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Input((28, 28, 1))        # Input image - input layer for encoder \n",
    "interdim = 256                  # This is the dimension of intermediate latent variable (after convolution and before embedding)\n",
    "latent_dim = 100                 # Dimension of the embedding space\n",
    "code = Input((latent_dim,))     # Input code - input layer for decoder\n",
    "depth = 64                      # \"Depth\" parameter for convolutional layers\n",
    "L = 70                          # Number of random projections (thetas)\n",
    "batchsize = 250                 # Size of one train batch\n",
    "epochs_num = 600                # Number of epochs to train the autoencoder model\n",
    "\n",
    "OUTPUT_DIR = os.path.join(\"results\", \"convolutional_swae\") \n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "CHECKPOINT_DIR = os.path.join(\"saved_models\", \"convolutional_swae\", \"checkpoints\") \n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def tile_images(image_stack):\n",
    "    assert len(image_stack.shape) == 3\n",
    "    image_list = [image_stack[i, :, :] for i in range(image_stack.shape[0])]\n",
    "    tiled_images = np.concatenate(image_list, axis=1)\n",
    "    return tiled_images\n",
    "\n",
    "\n",
    "def save_predicted_images(generator_model, output_dir, epoch, frechet_distance):\n",
    "    dir_path = os.path.join(output_dir, \"Epoch_\" + str(epoch))\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "    test_image_stack = generator_model.predict(np.random.normal(size=(20, 100)))\n",
    "    test_image_stack = (test_image_stack * 127.5) + 127.5\n",
    "    test_image_stack = np.squeeze(np.round(test_image_stack).astype(np.uint8))\n",
    "    tiled_output = tile_images(test_image_stack)\n",
    "    tiled_output = Image.fromarray(tiled_output, mode='L')  # L specifies greyscale\n",
    "    outfile = os.path.join(dir_path, 'epoch_{}.png'.format(epoch))\n",
    "    tiled_output.save(outfile)\n",
    "    \n",
    "    fd_json = {\"frechet_distance\": \"%.4f\" % frechet_distance.numpy()}\n",
    "    with open(os.path.join(dir_path, \"frechet_distance.json\"), \"w\") as f:        \n",
    "        json.dump(fd_json, f, indent=4)\n",
    "    \n",
    "    \n",
    "def save_generator_model(generator_model, checkpoint_dir, epoch, frechet_distance):\n",
    "    dir_path = os.path.join(checkpoint_dir, \"Epoch_\" + str(epoch))\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        \n",
    "    fd_json = {\"frechet_distance\": \"%.4f\" % frechet_distance.numpy()}\n",
    "    with open(os.path.join(dir_path, \"frechet_distance.json\"), \"w\") as f:        \n",
    "        json.dump(fd_json, f, indent=4)\n",
    "    \n",
    "    generator_model.save(os.path.join(dir_path, \"generator_model.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SWAEEncoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 2,222,116\n",
      "Trainable params: 2,220,452\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.variable_scope(\"swae_encoder\"):\n",
    "    x = Conv2D(depth * 1, kernel_size=(3, 3), padding=\"same\")(img)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(depth * 1, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = AveragePooling2D(pool_size=(2, 2), padding=\"same\")(x)\n",
    "    x = Conv2D(depth * 2, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Conv2D(depth * 2, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = AveragePooling2D(pool_size=(2, 2), padding=\"same\")(x)\n",
    "    x = Conv2D(depth * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Conv2D(depth * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(2, 2), padding=\"same\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(interdim, activation='relu')(x)\n",
    "    encoded = Dense(latent_dim)(x)\n",
    "swae_encoder = Model(img, encoded, name=\"SWAEEncoder\")\n",
    "swae_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SWAEDecoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 128)       295040    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 1)         1153      \n",
      "=================================================================\n",
      "Total params: 3,901,569\n",
      "Trainable params: 3,892,097\n",
      "Non-trainable params: 9,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.variable_scope(\"swae_decoder\"):\n",
    "    x = Dense(interdim)(code)\n",
    "    x = Dense(depth * 64, activation='relu')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Reshape((4, 4, depth * 4))(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(depth * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(depth * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(depth * 4, kernel_size=(3, 3), padding=\"valid\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Conv2D(depth * 4, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(depth * 2, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    x = Conv2D(depth * 2, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    decoded = Conv2D(1, kernel_size=(3, 3), padding=\"same\", activation='sigmoid')(x)\n",
    "swae_decoder = Model(code, decoded, name=\"SWAEDecoder\")\n",
    "swae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Keras variables for theta and q_z samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_var = K.variable(generateTheta(L, latent_dim))\n",
    "q_z_sample_var = K.variable(generateQZ(batchsize, latent_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SWAE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "SWAEEncoder (Model)          (None, 100)               2222116   \n",
      "_________________________________________________________________\n",
      "SWAEDecoder (Model)          (None, 28, 28, 1)         3901569   \n",
      "=================================================================\n",
      "Total params: 6,123,685\n",
      "Trainable params: 6,112,549\n",
      "Non-trainable params: 11,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_encoded = swae_encoder(img)\n",
    "z_decoded = swae_decoder(img_encoded)\n",
    "swae_autoencoder = Model(img, z_decoded, name=\"SWAE\")\n",
    "swae_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_encoded_samples = K.dot(img_encoded, K.transpose(theta_var))  # projection of the encoded samples\n",
    "proj_q_z = K.dot(q_z_sample_var, K.transpose(theta_var))           # projection of ideal q_z samples\n",
    "\n",
    "# Calculate Sliced Wasserstein distance by sorting the projections and calculating L2 distance\n",
    "sliced_wass_dist = (tf.nn.top_k(tf.transpose(proj_encoded_samples), k=batchsize).values -\n",
    "                    tf.nn.top_k(tf.transpose(proj_q_z), k=batchsize).values) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_coeff = K.variable(10.0)\n",
    "\n",
    "cross_entropy_loss = (1.0) * K.mean(K.binary_crossentropy(K.flatten(img), K.flatten(z_decoded)))\n",
    "L1_loss = (1.0) * K.mean(K.abs(K.flatten(img) - K.flatten(z_decoded)))\n",
    "sliced_wass_loss = lambda_coeff * K.mean(sliced_wass_dist)\n",
    "\n",
    "swae_loss = L1_loss + cross_entropy_loss + sliced_wass_loss\n",
    "# swae_loss = cross_entropy_loss + sliced_wass_loss\n",
    "swae_autoencoder.add_loss(swae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py:819: UserWarning: Output SWAEDecoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to SWAEDecoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "swae_autoencoder.compile(optimizer='adam', loss='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.expand_dims(x_train.astype('float32') / 255., 3)\n",
    "\n",
    "div_coeff = 5 / 6\n",
    "train_num = int(len(x_train) * div_coeff)\n",
    "_x_train, x_fid_test = x_train[:train_num - 1], x_train[train_num:]\n",
    "x_train = _x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(x_train[30, ...]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_size = 28\n",
    "def plot_digits(*args):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    figure = np.zeros((digit_size * len(args), digit_size * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(len(args)):\n",
    "            figure[j * digit_size: (j + 1) * digit_size,\n",
    "                   i * digit_size: (i + 1) * digit_size] = args[j][i].squeeze()\n",
    "\n",
    "    plt.figure(figsize=(2 * n, 2 * len(args)))\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.grid(False)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n",
      "Epoch: [0], Batch: [198/199], Loss: 0.4425\n",
      "FID: 22.7726\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 22.7726\n",
      "\n",
      "\n",
      "Epoch: [1], Batch: [198/199], Loss: 0.3575\n",
      "Epoch: [2], Batch: [198/199], Loss: 0.2966\n",
      "Epoch: [3], Batch: [198/199], Loss: 0.3563\n",
      "Epoch: [4], Batch: [198/199], Loss: 0.2904\n",
      "Epoch: [5], Batch: [198/199], Loss: 0.2606\n",
      "FID: 15.4517\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 15.4517\n",
      "\n",
      "\n",
      "Epoch: [6], Batch: [198/199], Loss: 0.2856\n",
      "Epoch: [7], Batch: [198/199], Loss: 0.3092\n",
      "Epoch: [8], Batch: [198/199], Loss: 0.2465\n",
      "Epoch: [9], Batch: [198/199], Loss: 0.2247\n",
      "Epoch: [10], Batch: [198/199], Loss: 0.2457\n",
      "FID: 13.3024\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 13.3024\n",
      "\n",
      "\n",
      "Epoch: [11], Batch: [198/199], Loss: 0.2140\n",
      "Epoch: [12], Batch: [198/199], Loss: 0.2155\n",
      "Epoch: [13], Batch: [198/199], Loss: 0.2244\n",
      "Epoch: [14], Batch: [198/199], Loss: 0.2244\n",
      "Epoch: [15], Batch: [198/199], Loss: 0.2049\n",
      "FID: 16.5999\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 16.5999\n",
      "\n",
      "\n",
      "Epoch: [16], Batch: [198/199], Loss: 0.2317\n",
      "Epoch: [17], Batch: [198/199], Loss: 0.2097\n",
      "Epoch: [18], Batch: [198/199], Loss: 0.2068\n",
      "Epoch: [19], Batch: [198/199], Loss: 0.1890\n",
      "Epoch: [20], Batch: [198/199], Loss: 0.1960\n",
      "FID: 15.0102\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 15.0102\n",
      "\n",
      "\n",
      "Epoch: [21], Batch: [198/199], Loss: 0.1965\n",
      "Epoch: [22], Batch: [198/199], Loss: 0.1921\n",
      "Epoch: [23], Batch: [198/199], Loss: 0.1794\n",
      "Epoch: [24], Batch: [198/199], Loss: 0.1885\n",
      "Epoch: [25], Batch: [198/199], Loss: 0.1730\n",
      "FID: 14.3852\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 14.3852\n",
      "\n",
      "\n",
      "Epoch: [26], Batch: [198/199], Loss: 0.1888\n",
      "Epoch: [27], Batch: [198/199], Loss: 0.2259\n",
      "Epoch: [28], Batch: [198/199], Loss: 0.1826\n",
      "Epoch: [29], Batch: [198/199], Loss: 0.1875\n",
      "Epoch: [30], Batch: [198/199], Loss: 0.1793\n",
      "FID: 19.1995\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 19.1995\n",
      "\n",
      "\n",
      "Epoch: [31], Batch: [198/199], Loss: 0.1641\n",
      "Epoch: [32], Batch: [198/199], Loss: 0.2014\n",
      "Epoch: [33], Batch: [198/199], Loss: 0.2037\n",
      "Epoch: [34], Batch: [198/199], Loss: 0.1988\n",
      "Epoch: [35], Batch: [198/199], Loss: 0.1622\n",
      "FID: 17.4019\n",
      "\n",
      "SAVING THE GENERATOR MODEL, FD: 17.4019\n",
      "\n",
      "\n",
      "Epoch: [36], Batch: [198/199], Loss: 0.1651\n",
      "Epoch: [37], Batch: [21/199], Loss: 0.1653"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sys import stdout\n",
    "from collections import namedtuple\n",
    "from tensorflow_gan.examples.mnist import util\n",
    "\n",
    "\n",
    "loss = []   # for loss graph\n",
    "fig = plt.figure()\n",
    "n_compare = 10\n",
    "#epochs_num_del_3 = epochs_num // 3\n",
    "epochs_to_change_lambda_coeff = 20\n",
    "\n",
    "epoch = 0\n",
    "num_of_good_epochs = 0\n",
    "\n",
    "frechet_distance_edges = [30, 20, 10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "frechet_distance_checkpoints = []\n",
    "FDCheckpoint = namedtuple(\"FDCheckpoint\", [\"dist\"])\n",
    "for edge in frechet_distance_edges:\n",
    "    frechet_distance_checkpoints.append(FDCheckpoint(dist=edge))\n",
    "    \n",
    "x_fid_test = tf.convert_to_tensor(x_fid_test, dtype=tf.float32)\n",
    "\n",
    "while True:\n",
    "#for epoch in range(epochs_num):\n",
    "    ind = np.random.permutation(x_train.shape[0])\n",
    "    \n",
    "    #if epoch > epochs_num_del_3:\n",
    "    if epoch > epochs_to_change_lambda_coeff:\n",
    "        K.set_value(lambda_coeff, 1.1 * K.eval(lambda_coeff))\n",
    "        \n",
    "    n_batches = int(x_train.shape[0] / batchsize)\n",
    "    min_loss_per_epoch = 100000\n",
    "    for i in range(n_batches):\n",
    "        next_batch = x_train[ind[i * batchsize: (i + 1) * batchsize], ...]\n",
    "        \n",
    "        theta_ = generateTheta(L, latent_dim)\n",
    "        q_z_sample_ = generateQZ(batchsize, latent_dim)\n",
    "        K.set_value(theta_var, theta_)\n",
    "        K.set_value(q_z_sample_var, q_z_sample_)\n",
    "        \n",
    "        cur_loss = swae_autoencoder.train_on_batch(x=next_batch, y=None)\n",
    "        if cur_loss < min_loss_per_epoch:\n",
    "            min_loss_per_epoch = cur_loss\n",
    "            \n",
    "        loss.append(cur_loss)\n",
    "        \n",
    "        stdout.write(\"\\rEpoch: [%d], Batch: [%d/%d], Loss: %.4f\" % (epoch, i, n_batches, cur_loss))\n",
    "        if i < n_batches - 1:\n",
    "            stdout.flush()\n",
    "#         else:\n",
    "#             stdout.write(\", Minimum loss per epoch: %.4f\\n\" % min_loss_per_epoch)\n",
    "            \n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         check_imgs = x_train[:n_compare]\n",
    "#         decoded_imgs = swae_autoencoder.predict(check_imgs)\n",
    "#         plot_digits(check_imgs, decoded_imgs)\n",
    "    if epoch % 5 == 0:\n",
    "        generated_images = swae_decoder.predict(np.random.normal(size=(len(x_fid_test), 100)))\n",
    "        generated_images = tf.convert_to_tensor(generated_images, dtype=tf.float32)\n",
    "\n",
    "        frechet_distance = util.mnist_frechet_distance(x_fid_test, generated_images)\n",
    "        stdout.write(\"\\nFID: %.4f\" % frechet_distance)\n",
    "\n",
    "        save_predicted_images(swae_decoder, OUTPUT_DIR, epoch, frechet_distance)\n",
    "\n",
    "        for i in range(len(frechet_distance_checkpoints)):\n",
    "            if frechet_distance <= frechet_distance_checkpoints[i].dist and frechet_distance >= frechet_distance_checkpoints[i + 1].dist:\n",
    "                stdout.write(\"\\n\\nSAVING THE GENERATOR MODEL, FD: %.4f\\n\\n\" % frechet_distance)\n",
    "                save_generator_model(swae_decoder, CHECKPOINT_DIR, epoch, frechet_distance)                    \n",
    "\n",
    "        if frechet_distance <= 0.01:\n",
    "            num_of_good_epochs += 1\n",
    "            if num_of_good_epochs >= 4:\n",
    "                break\n",
    "        else:\n",
    "            num_of_good_epochs = 0\n",
    "    stdout.write(\"\\n\")\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE MODELS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "save_path = os.path.join(\"saved_models\", \"convolutional_swae\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "with open(os.path.join(save_path, \"ae_%d_ldim_%d_epochs.json\" % (latent_dim, epochs_num)), \"w\") as f:\n",
    "    json.dump(swae_autoencoder.to_json(), f, indent=4)\n",
    "swae_autoencoder.save_weights(os.path.join(save_path, \"ae_%d_ldim_%d_epochs.h5\") % (latent_dim, epochs_num))\n",
    "\n",
    "with open(os.path.join(save_path, \"ae_encoder_%d_ldim_%d_epochs.json\" % (latent_dim, epochs_num)), \"w\") as f:\n",
    "    json.dump(swae_encoder.to_json(), f, indent=4)\n",
    "swae_encoder.save_weights(os.path.join(save_path, \"ae_encoder_%d_ldim_%d_epochs.h5\" % (latent_dim, epochs_num)))\n",
    "\n",
    "with open(os.path.join(save_path, \"ae_decoder_%d_ldim_%d_epochs.json\" % (latent_dim, epochs_num)), \"w\") as f:\n",
    "    json.dump(swae_decoder.to_json(), f, indent=4)\n",
    "swae_decoder.save_weights(os.path.join(save_path, \"ae_decoder_%d_ldim_%d_epochs.h5\" % (latent_dim, epochs_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random samples from q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare = []\n",
    "for i in range(10):\n",
    "    codes = generateQZ(n_compare, latent_dim)\n",
    "    predicted = swae_decoder.predict(codes)\n",
    "    to_compare.append(predicted[:n_compare])\n",
    "plot_digits(*to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "random_samples = generateQZ(n_sample ** 2, latent_dim)\n",
    "random_decode_img = np.squeeze(swae_decoder.predict(random_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_random = np.zeros((n_sample * 28, n_sample * 28))\n",
    "count = 0\n",
    "for i in range(n_sample):\n",
    "    for j in range(n_sample):        \n",
    "        img_random[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = random_decode_img[count, ...]\n",
    "        count += 1\n",
    "        \n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_random, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.plot(np.asarray(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
